https://discourse.onlinedegree.iitm.ac.in/t/171141

There’s also really no way to predict an edge case caused by an LLM.</p>
<p>Some might suggest using <code>temperature=0</code> to get more deterministic LLM behavior — and while true to an extent, it does little to encourage exploration, especially in tasks that require self-correction based on environmental feedback. Prompt engineering too wouldn’t be helpful here as 4o-mini isn’t all that great at 0-shot instruction following, especially when the system prompt is already saturated with 50+ fine-grained instructions. There’s only so much stuff it can pay attention to.</p>
<p><strong>Hardcoded tool agents also aren’t really “agents” in my view— they’re more like passive AI powered regex matchers</strong>: merely mapping inputs to functions by inferring from context window. That puts all the burden of answering on the hardcoded functions, leaving the agent itself uninvolved in the solutioning process.